import os
import asyncio
import logging
import threading
import time
import datetime
from collections import deque
from typing import List, Dict, Any, Optional
import json
import websockets

from fastapi import FastAPI, Body, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import requests
import uvicorn

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# åŠ è½½JSONé…ç½®
class ConfigLoader:
    _instance = None
    _config = None
    
    @classmethod
    def get_config(cls, config_path='config.json'):
        if cls._config is None:
            cls._load_config(config_path)
        return cls._config
    
    @classmethod
    def _load_config(cls, config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            
            # æ‰å¹³åŒ–é…ç½®ï¼Œä¾¿äºè®¿é—®
            cls._config = {
                'port': config_data['server']['port'],
                'node_capacity': config_data['ring']['node_capacity'],
                'patch_lifetime': config_data['ring']['patch_lifetime'],
                'ring_size': config_data['ring']['ring_size'],
                'consume_check_url': config_data['consumption']['check_url'],
                'consume_check_timeout': config_data['consumption']['check_timeout'],
                'max_retry_count': config_data['consumption']['max_retry_count'],
                'retry_interval': config_data['consumption']['retry_interval'],
                'workers': config_data['server']['workers'],
                'limit_concurrency': config_data['server']['limit_concurrency'],
                'backlog': config_data['server']['backlog'],
                'reload': config_data['server']['reload'],
                'batch_size': config_data['queue']['batch_size'],
                'empty_sleep_time': config_data['queue']['empty_sleep_time']
            }
            logger.info(f"é…ç½®åŠ è½½æˆåŠŸ: {cls._config}")
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
            # ä½¿ç”¨é»˜è®¤é…ç½®ä½œä¸ºåå¤‡
            cls._config = {
                'port': 8000,
                'node_capacity': 100,
                'patch_lifetime': 8,
                'ring_size': 10,
                'consume_check_url': "http://localhost:2345/api/consumption-status",
                'consume_check_timeout': 5,
                'max_retry_count': 99,
                'retry_interval': 1,
                'workers': 'auto',
                'limit_concurrency': 1000,
                'backlog': 2048,
                'reload': True,
                'batch_size': 100,
                'empty_sleep_time': 0.01
            }
            logger.warning(f"ä½¿ç”¨é»˜è®¤é…ç½®: {cls._config}")


# è·å–é…ç½®
config = ConfigLoader.get_config()

class RingNode:
    def __init__(self):
        # ä½¿ç”¨å›ºå®šé•¿åº¦çš„åŒç«¯é˜Ÿåˆ—å­˜å‚¨å¼¹å¹•ï¼Œè‡ªåŠ¨ä¸¢å¼ƒè€å¼¹å¹•
        self.danmakus = deque(maxlen=config['node_capacity'])
        self.create_time = time.time()  # åˆ›å»ºæ—¶é—´
        self.next: Optional[RingNode] = None  # æŒ‡å‘ä¸‹ä¸€ä¸ªèŠ‚ç‚¹


class DualBucketSystem:
    def __init__(self, bucket_capacity=config['node_capacity'], bucket_lifetime=config['patch_lifetime']):
        self.bucket_capacity = bucket_capacity
        self.bucket_lifetime = bucket_lifetime
        self.lock = threading.Lock()

        # ä¸¤ä¸ªbucketï¼šAå’ŒB
        self.bucket_a = {
            'danmakus': deque(maxlen=bucket_capacity),
            'start_time': time.time(),
            'is_active': True,  # Aæ¡¶å½“å‰æ˜¯å¦æ´»è·ƒ
            'is_consuming': False  # Aæ¡¶æ˜¯å¦æ­£åœ¨æ¶ˆè´¹
        }

        self.bucket_b = {
            'danmakus': deque(maxlen=bucket_capacity),
            'start_time': None,  # Bæ¡¶åœ¨Aæ¡¶å‘é€åæ‰å¼€å§‹è®¡æ—¶
            'is_active': False,  # Bæ¡¶å½“å‰æ˜¯å¦æ´»è·ƒ
            'is_consuming': False  # Bæ¡¶æ˜¯å¦æ­£åœ¨æ¶ˆè´¹
        }

        self.current_bucket = self.bucket_a  # å½“å‰æ´»è·ƒçš„bucket

    def add_danmaku(self, danmaku: Dict[str, Any]):
        """æ·»åŠ å¼¹å¹•åˆ°å½“å‰æ´»è·ƒçš„bucketï¼Œå¦‚æœå½“å‰bucketæ­£åœ¨æ¶ˆè´¹ï¼Œåˆ™åˆ‡æ¢åˆ°å¦ä¸€ä¸ªbucket"""
        with self.lock:
            # å¦‚æœå½“å‰bucketæ­£åœ¨æ¶ˆè´¹ï¼Œåˆ‡æ¢åˆ°å¦ä¸€ä¸ªbucket
            if self.current_bucket['is_consuming']:
                # æ‰¾åˆ°å¦ä¸€ä¸ªå¯ç”¨çš„bucket
                if self.current_bucket == self.bucket_a:
                    # Aæ¡¶æ­£åœ¨æ¶ˆè´¹ï¼Œåˆ‡æ¢åˆ°Bæ¡¶
                    if not self.bucket_b['is_active']:
                        # æ¿€æ´»Bæ¡¶å¹¶å¼€å§‹è®¡æ—¶
                        self.bucket_b['is_active'] = True
                        self.bucket_b['start_time'] = time.time()
                    self.current_bucket = self.bucket_b
                    logger.info(f"ğŸ”„ Aæ¡¶æ­£åœ¨æ¶ˆè´¹ï¼Œåˆ‡æ¢åˆ°Bæ¡¶æ¥æ”¶æ–°å¼¹å¹•")
                else:
                    # Bæ¡¶æ­£åœ¨æ¶ˆè´¹ï¼Œåˆ‡æ¢åˆ°Aæ¡¶
                    if not self.bucket_a['is_active']:
                        # æ¿€æ´»Aæ¡¶å¹¶å¼€å§‹è®¡æ—¶
                        self.bucket_a['is_active'] = True
                        self.bucket_a['start_time'] = time.time()
                    self.current_bucket = self.bucket_a
                    logger.info(f"ğŸ”„ Bæ¡¶æ­£åœ¨æ¶ˆè´¹ï¼Œåˆ‡æ¢åˆ°Aæ¡¶æ¥æ”¶æ–°å¼¹å¹•")

            # æ·»åŠ åˆ°å½“å‰æ´»è·ƒçš„bucketï¼ˆå¯èƒ½æ˜¯åˆ‡æ¢åçš„bucketï¼‰
            self.current_bucket['danmakus'].append(danmaku)
            logger.debug(f"ğŸ“¥ å¼¹å¹•å·²æ·»åŠ åˆ°{'A' if self.current_bucket == self.bucket_a else 'B'}æ¡¶: {danmaku['content'][:30]}...")

    def get_consumable_bucket(self):
        """è·å–å¯æ¶ˆè´¹çš„bucket"""
        with self.lock:
            current_time = time.time()

            # æ£€æŸ¥Aæ¡¶æ˜¯å¦åˆ°æœŸä¸”æœªåœ¨æ¶ˆè´¹
            if (self.bucket_a['is_active'] and
                not self.bucket_a['is_consuming'] and
                self.bucket_a['danmakus'] and
                current_time - self.bucket_a['start_time'] > self.bucket_lifetime):
                return self.bucket_a

            # æ£€æŸ¥Bæ¡¶æ˜¯å¦åˆ°æœŸä¸”æœªåœ¨æ¶ˆè´¹ï¼ˆBæ¡¶å·²å¼€å§‹è®¡æ—¶ï¼‰
            if (self.bucket_b['is_active'] and
                not self.bucket_b['is_consuming'] and
                self.bucket_b['danmakus'] and
                self.bucket_b['start_time'] is not None and
                current_time - self.bucket_b['start_time'] > self.bucket_lifetime):
                return self.bucket_b

            return None

    def mark_bucket_consuming(self, bucket):
        """æ ‡è®°bucketæ­£åœ¨æ¶ˆè´¹"""
        with self.lock:
            bucket['is_consuming'] = True

    def switch_bucket(self):
        """åˆ‡æ¢bucketï¼šå½“å‰bucketæ¶ˆè´¹å®Œæˆåï¼Œåˆ‡æ¢åˆ°å¦ä¸€ä¸ªbucket"""
        with self.lock:
            if self.current_bucket == self.bucket_a:
                # Aæ¡¶æ¶ˆè´¹å®Œæˆï¼Œåˆ‡æ¢åˆ°Bæ¡¶
                logger.info("ğŸ”„ Aæ¡¶æ¶ˆè´¹å®Œæˆï¼Œåˆ‡æ¢åˆ°Bæ¡¶")
                self.bucket_a['is_active'] = False
                self.bucket_a['is_consuming'] = False
                self.bucket_a['danmakus'].clear()

                # Bæ¡¶å¼€å§‹è®¡æ—¶
                self.bucket_b['is_active'] = True
                self.bucket_b['start_time'] = time.time()
                self.current_bucket = self.bucket_b

            else:
                # Bæ¡¶æ¶ˆè´¹å®Œæˆï¼Œåˆ‡æ¢å›Aæ¡¶
                logger.info("ğŸ”„ Bæ¡¶æ¶ˆè´¹å®Œæˆï¼Œåˆ‡æ¢åˆ°Aæ¡¶")
                self.bucket_b['is_active'] = False
                self.bucket_b['is_consuming'] = False
                self.bucket_b['danmakus'].clear()
                self.bucket_b['start_time'] = None

                # Aæ¡¶é‡æ–°å¼€å§‹è®¡æ—¶
                self.bucket_a['is_active'] = True
                self.bucket_a['start_time'] = time.time()
                self.current_bucket = self.bucket_a

    def get_merged_danmaku(self, bucket):
        """åˆå¹¶bucketä¸­çš„æ‰€æœ‰å¼¹å¹•"""
        with self.lock:
            if not bucket['danmakus']:
                return None

            # æå–æ‰€æœ‰å¼¹å¹•å†…å®¹å¹¶åˆå¹¶
            contents = [dm['content'] for dm in bucket['danmakus']]
            merged_content = '\n'.join(contents)

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¼¹å¹•çš„ç±»å‹ä½œä¸ºåˆå¹¶åçš„ç±»å‹
            first_danmaku = next(iter(bucket['danmakus']), {})
            merged_danmaku = {
                'type': 'message',
                'content': merged_content,
                'danmu_type': first_danmaku.get('danmu_type', 'danmaku'),
                'count': len(bucket['danmakus'])
            }

            return merged_danmaku


# 4. åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
app = FastAPI(
    title="Danmaku Proxy Service",
    description="æç®€å¼¹å¹•ä»£ç†æœåŠ¡",
    version="1.0.0"
)

# 5. é…ç½®CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# 6. å®šä¹‰è¯·æ±‚æ¨¡å‹
class DanmakuRequest(BaseModel):
    type: str = Field(..., description="æ¶ˆæ¯ç±»å‹")
    content: str = Field(..., description="å¼¹å¹•å†…å®¹")
    danmu_type: str = Field(..., description="å¼¹å¹•ç±»å‹")


# 7. åˆ›å»ºåŒbucketç³»ç»Ÿå®ä¾‹
bucket_system = DualBucketSystem()

# åˆ›å»ºä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å¼‚æ­¥é˜Ÿåˆ—
class AsyncDanmakuQueue:
    def __init__(self):
        self.queue = deque()
        self.lock = asyncio.Lock()

    async def put(self, item):
        async with self.lock:
            self.queue.append(item)

    async def get_batch(self, max_items=config['batch_size']):
        batch = []
        async with self.lock:
            while self.queue and len(batch) < max_items:
                batch.append(self.queue.popleft())
        return batch

# ä»˜è´¹æ¶ˆæ¯é˜Ÿåˆ— - åªå¤„ç†ä»˜è´¹æ¶ˆæ¯
class PaidDanmakuQueue:
    def __init__(self):
        self.paid_queue = deque()  # åªä¿ç•™ä»˜è´¹æ¶ˆæ¯é˜Ÿåˆ—
        self.lock = asyncio.Lock()
        # ä»˜è´¹æ¶ˆæ¯ç±»å‹
        self.paid_types = {'super_chat', 'gift', 'buy_guard'}

    def is_paid_message(self, danmu_type: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä»˜è´¹æ¶ˆæ¯"""
        return danmu_type in self.paid_types

    async def put(self, item):
        async with self.lock:
            if self.is_paid_message(item.get('danmu_type', '')):
                self.paid_queue.append(item)
                logger.info(f"ğŸ’° ä»˜è´¹æ¶ˆæ¯å·²æ·»åŠ åˆ°ä¼˜å…ˆé˜Ÿåˆ—: {item.get('danmu_type')} - {item.get('content', '')[:30]}...")
                return True
            return False  # æ™®é€šæ¶ˆæ¯ä¸å¤„ç†ï¼Œè¿”å›False

    async def get_paid_message(self):
        """è·å–å•æ¡ä»˜è´¹æ¶ˆæ¯"""
        async with self.lock:
            if self.paid_queue:
                return self.paid_queue.popleft()
            return None

    def has_paid_messages(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰ä»˜è´¹æ¶ˆæ¯"""
        return len(self.paid_queue) > 0

# åˆ›å»ºå…¨å±€é˜Ÿåˆ—å®ä¾‹
danmaku_queue = PaidDanmakuQueue()

# é˜Ÿåˆ—çŠ¶æ€ç›‘æ§å‡½æ•°
async def monitor_queue_status():
    """ç›‘æ§ä»˜è´¹æ¶ˆæ¯é˜Ÿåˆ—çŠ¶æ€"""
    while True:
        try:
            # æ¯30ç§’è®°å½•ä¸€æ¬¡é˜Ÿåˆ—çŠ¶æ€
            await asyncio.sleep(30)

            # åªç›‘æ§ä»˜è´¹æ¶ˆæ¯é˜Ÿåˆ—
            paid_count = len(danmaku_queue.paid_queue)

            if paid_count > 0:
                logger.info(f"ğŸ“Š ä»˜è´¹æ¶ˆæ¯é˜Ÿåˆ—: {paid_count}æ¡")

                # å¦‚æœä»˜è´¹æ¶ˆæ¯ç§¯å‹è¿‡å¤šï¼Œå‘å‡ºè­¦å‘Š
                if paid_count > 10:
                    logger.warning(f"âš ï¸ ä»˜è´¹æ¶ˆæ¯ç§¯å‹ä¸¥é‡: {paid_count}æ¡")

        except Exception as e:
            logger.error(f"ç›‘æ§é˜Ÿåˆ—çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")

# WebSocketè¿æ¥ç®¡ç†å™¨
class WebSocketManager:
    def __init__(self):
        self.connections = set()
        self.lock = asyncio.Lock()

    async def connect(self, websocket):
        async with self.lock:
            self.connections.add(websocket)

    async def disconnect(self, websocket):
        async with self.lock:
            self.connections.discard(websocket)

    async def broadcast(self, message):
        """å‘æ‰€æœ‰è¿æ¥çš„WebSocketå®¢æˆ·ç«¯å¹¿æ’­æ¶ˆæ¯"""
        if not self.connections:
            return

        disconnected = set()
        async with self.lock:
            for websocket in self.connections:
                try:
                    await websocket.send(json.dumps(message))
                except websockets.exceptions.ConnectionClosed:
                    disconnected.add(websocket)
                except Exception as e:
                    logger.error(f"WebSocketå¹¿æ’­æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
                    disconnected.add(websocket)

            # ç§»é™¤æ–­å¼€çš„è¿æ¥
            for websocket in disconnected:
                self.connections.discard(websocket)

# åˆ›å»ºWebSocketç®¡ç†å™¨å®ä¾‹
ws_manager = WebSocketManager()

# å…¨å±€äº‹ä»¶å¾ªç¯ç®¡ç†å™¨
class EventLoopManager:
    def __init__(self):
        self.loop = None
        self._lock = threading.Lock()

    def get_loop(self):
        """è·å–æˆ–åˆ›å»ºäº‹ä»¶å¾ªç¯"""
        with self._lock:
            if self.loop is None or self.loop.is_closed():
                try:
                    self.loop = asyncio.get_event_loop()
                except RuntimeError:
                    self.loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(self.loop)
            return self.loop

    def run_coroutine(self, coro):
        """åœ¨çº¿ç¨‹ä¸­è¿è¡Œåç¨‹"""
        loop = self.get_loop()
        if loop.is_running():
            # å¦‚æœå¾ªç¯å·²ç»åœ¨è¿è¡Œï¼Œåˆ›å»ºä»»åŠ¡
            return asyncio.run_coroutine_threadsafe(coro, loop)
        else:
            # å¦‚æœå¾ªç¯æ²¡æœ‰è¿è¡Œï¼Œç›´æ¥è¿è¡Œ
            return loop.run_until_complete(coro)

event_loop_manager = EventLoopManager()


# ä¿®æ”¹process_danmaku_batchå‡½æ•°ï¼Œå¤„ç†ä»˜è´¹æ¶ˆæ¯å¹¶ç›‘æ§æ™®é€šæ¶ˆæ¯
async def process_danmaku_batch():
    """åå°ä»»åŠ¡ï¼šå¤„ç†ä»˜è´¹æ¶ˆæ¯ï¼Œæ™®é€šæ¶ˆæ¯ç”±consume_asyncå¤„ç†"""
    while True:
        # é¦–å…ˆæ£€æŸ¥ä»˜è´¹æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        paid_message = await danmaku_queue.get_paid_message()
        
        if paid_message:
            # ä»˜è´¹æ¶ˆæ¯ç›´æ¥å‘é€ï¼Œä¸æ£€æŸ¥æ¶ˆè´¹çŠ¶æ€ï¼ˆä¼˜å…ˆå¤„ç†ï¼‰
            try:
                consume_message = {
                    'type': 'message',
                    'content': paid_message['content'],
                    'danmu_type': paid_message['danmu_type'],
                    'timestamp': datetime.datetime.now().isoformat(),
                    'priority': 'high'  # æ ‡è®°ä¸ºä¼˜å…ˆæ¶ˆæ¯
                }

                # ç›´æ¥å‘é€åˆ°ä¸»æœåŠ¡
                await send_to_main_server(consume_message)
                logger.info(f"ğŸ’° ä»˜è´¹æ¶ˆæ¯ç›´æ¥å‘é€æˆåŠŸ: {paid_message['danmu_type']} - {paid_message['content'][:50]}...")

                # ä»˜è´¹æ¶ˆæ¯å¤„ç†å®Œåç»§ç»­å¾ªç¯ï¼Œç¡®ä¿ä¸‹ä¸€æ¡ä¹Ÿä¼˜å…ˆå¤„ç†ä»˜è´¹æ¶ˆæ¯
                await asyncio.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹
                continue

            except Exception as e:
                logger.error(f"ä»˜è´¹æ¶ˆæ¯å‘é€å¤±è´¥: {str(e)}")

        # æ²¡æœ‰ä»˜è´¹æ¶ˆæ¯æ—¶çŸ­æš‚ä¼‘çœ ï¼Œè®©å‡ºCPUç»™æ™®é€šæ¶ˆæ¯å¤„ç†
        await asyncio.sleep(config['empty_sleep_time'])  # 10ms


# 8. å®šä¹‰æ¶ˆè´¹å‡½æ•°ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
async def consume_async():
    """å¼‚æ­¥æ¶ˆè´¹è¿‡æœŸçš„å¼¹å¹•æ•°æ®ï¼Œä¼šå…ˆæ£€æŸ¥è¿œç¨‹æœåŠ¡æ˜¯å¦å…è®¸æ¶ˆè´¹ï¼Œç„¶åå‘é€åˆ°ä¸»æœåŠ¡æ¥å£
    ä¼˜å…ˆå¤„ç†ä»˜è´¹æ¶ˆæ¯ï¼Œç„¶åå¤„ç†æ™®é€šæ¶ˆæ¯çš„bucket"""
    while True:
        await asyncio.sleep(1)  # å¼‚æ­¥ç­‰å¾…1ç§’

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä»˜è´¹æ¶ˆæ¯éœ€è¦å¤„ç†
        if danmaku_queue.has_paid_messages():
            logger.info("æ£€æµ‹åˆ°ä»˜è´¹æ¶ˆæ¯ï¼Œä¼˜å…ˆå¤„ç†ä»˜è´¹æ¶ˆæ¯é˜Ÿåˆ—")
            # ä»˜è´¹æ¶ˆæ¯ç”±process_danmaku_batchå‡½æ•°å¤„ç†ï¼Œè¿™é‡Œä¸é‡å¤å¤„ç†
            continue

        # æ£€æŸ¥è¿œç¨‹æœåŠ¡æ˜¯å¦å…è®¸æ¶ˆè´¹
        if not can_consume():
            logger.info("Remote service indicates consumption is not allowed at this time")
            continue

        # åŒbucketäº¤æ›¿å·¥ä½œæœºåˆ¶ - åªåœ¨æ²¡æœ‰ä»˜è´¹æ¶ˆæ¯æ—¶å¤„ç†
        consumable_bucket = bucket_system.get_consumable_bucket()
        if consumable_bucket:
            # æ ‡è®°bucketä¸ºæ­£åœ¨æ¶ˆè´¹çŠ¶æ€
            bucket_system.mark_bucket_consuming(consumable_bucket)

            # è·å–åˆå¹¶åçš„å¼¹å¹•æ•°æ®
            merged_danmaku = bucket_system.get_merged_danmaku(consumable_bucket)
            if merged_danmaku:
                bucket_name = "A" if consumable_bucket == bucket_system.bucket_a else "B"
                logger.info(f"ğŸ”„ åŒbucketäº¤æ›¿å·¥ä½œ - æ¶ˆè´¹{bucket_name}æ¡¶å¼¹å¹•: {merged_danmaku['content'][:50]}...")

                # æ„å»ºæ¶ˆè´¹æ¶ˆæ¯
                consume_message = {
                    'type': 'message',
                    'content': merged_danmaku['content'],
                    'danmu_type': merged_danmaku['danmu_type'],
                    'timestamp': datetime.datetime.now().isoformat()
                }

                # å‘é€åˆ°ä¸»æœåŠ¡
                try:
                    await send_to_main_server(consume_message)
                    logger.info(f"âœ… æˆåŠŸå‘é€åˆ°ä¸»æœåŠ¡: {consume_message}")

                    # åˆ‡æ¢bucketï¼ˆåœ¨switch_bucketä¸­å¤„ç†ï¼‰
                    bucket_system.switch_bucket()

                except Exception as e:
                    logger.error(f"å‘é€åˆ°ä¸»æœåŠ¡å¤±è´¥: {e}", exc_info=True)
                    # æ¶ˆè´¹å¤±è´¥ä¹Ÿè¦åˆ‡æ¢bucket
                    bucket_system.switch_bucket()
        else:
            logger.debug("æš‚æ— å¯æ¶ˆè´¹èŠ‚ç‚¹ï¼Œç»§ç»­ç­‰å¾…...")


# æ·»åŠ æ£€æŸ¥è¿œç¨‹æœåŠ¡æ˜¯å¦å…è®¸æ¶ˆè´¹çš„å‡½æ•°
def can_consume():
    """
    æ£€æŸ¥è¿œç¨‹æœåŠ¡æ˜¯å¦å…è®¸æ¶ˆè´¹å¼¹å¹•ï¼Œè¯·æ±‚ä¸»æœåŠ¡çš„/api/consumption-statusæ¥å£
    è¿”å›: bool - Trueè¡¨ç¤ºå…è®¸æ¶ˆè´¹ï¼ŒFalseè¡¨ç¤ºä¸å…è®¸æ¶ˆè´¹
    """
    retry_count = 0
    while retry_count <= config['max_retry_count']:
        try:
            # å‘é€GETè¯·æ±‚åˆ°ä¸»æœåŠ¡çš„æ¶ˆè´¹çŠ¶æ€æ¥å£
            response = requests.get(
                config['consume_check_url'],
                timeout=config['consume_check_timeout'],
                headers={'Content-Type': 'application/json'}
            )

            # æ£€æŸ¥å“åº”çŠ¶æ€ç 
            if response.status_code == 200:
                # è§£æJSONå“åº”
                data = response.json()
                # ä¸»æœåŠ¡è¿”å›çš„JSONåŒ…å«'can_consume'å¸ƒå°”å­—æ®µï¼ŒåŸºäºisLLMWorkingçŠ¶æ€
                can_consume_result = data.get('can_consume', False)
                logger.info(f"ä¸»æœåŠ¡æ¶ˆè´¹çŠ¶æ€æ£€æŸ¥æˆåŠŸ: can_consume={can_consume_result}")
                return can_consume_result
            else:
                logger.warning(f"ä¸»æœåŠ¡è¿”å›é200çŠ¶æ€ç : {response.status_code}")
                retry_count += 1
                time.sleep(config['retry_interval'])
        except requests.exceptions.RequestException as e:
            # å¤„ç†è¯·æ±‚å¼‚å¸¸
            logger.error(f"è¿æ¥ä¸»æœåŠ¡å¤±è´¥: {str(e)}")
            retry_count += 1
            time.sleep(config['retry_interval'])
        except json.JSONDecodeError as e:
            logger.error(f"è§£æä¸»æœåŠ¡å“åº”JSONå¤±è´¥: {str(e)}")
            retry_count += 1
            time.sleep(config['retry_interval'])

    # é‡è¯•æ¬¡æ•°è€—å°½ï¼Œé»˜è®¤è¿”å›Falseè¡¨ç¤ºä¸å…è®¸æ¶ˆè´¹
    logger.error(f"æ£€æŸ¥æ¶ˆè´¹çŠ¶æ€å¤±è´¥ï¼Œé‡è¯•{config['max_retry_count']}æ¬¡åæ”¾å¼ƒ")
    return False


# æ–°å¢ï¼šå‘é€å¼¹å¹•æ¶ˆæ¯åˆ°ä¸»æœåŠ¡
async def send_to_main_server(message: dict):
    """
    å¼‚æ­¥å‘é€å¼¹å¹•æ¶ˆæ¯åˆ°ä¸»æœåŠ¡çš„æ¶ˆè´¹æ¥å£ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
    """
    retry_count = 0
    max_retries = config['main_server_max_retry_count']

    while retry_count <= max_retries:
        try:
            # ä¸»æœåŠ¡çš„æ¶ˆè´¹æ¥å£URL
            main_server_url = config['main_server_consume_url']
            headers = {"Content-Type": "application/json"}
            timeout = config['main_server_timeout']

            # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯å‘é€è¯·æ±‚
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    main_server_url,
                    json=message,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=timeout)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result.get('success'):
                            logger.info(f"å¼¹å¹•æ¶ˆæ¯æˆåŠŸå‘é€åˆ°ä¸»æœåŠ¡: {message.get('content', '')[:50]}...")
                            return True
                        else:
                            logger.error(f"ä¸»æœåŠ¡å¤„ç†å¤±è´¥: {result.get('message', 'Unknown error')}")
                            return False
                    else:
                        logger.error(f"ä¸»æœåŠ¡è¿”å›é”™è¯¯çŠ¶æ€ç : {response.status}")
                        return False

        except Exception as e:
            retry_count += 1
            if retry_count <= max_retries:
                logger.warning(f"å‘é€åˆ°ä¸»æœåŠ¡å¤±è´¥ (å°è¯• {retry_count}/{max_retries + 1}): {str(e)}")
                await asyncio.sleep(config['main_server_retry_interval'])
            else:
                logger.error(f"å‘é€åˆ°ä¸»æœåŠ¡å¤±è´¥ï¼Œé‡è¯•æ¬¡æ•°è€—å°½: {str(e)}", exc_info=True)
                # å¯ä»¥é€‰æ‹©é‡è¯•æˆ–è®°å½•å¤±è´¥çš„æ¶ˆæ¯
                raise e

    return False


# 9. å¯åŠ¨æ¶ˆè´¹è€…çº¿ç¨‹å’Œåå°ä»»åŠ¡
@app.on_event("startup")
async def startup_event():
    # å¯åŠ¨é˜Ÿåˆ—ç›‘æ§ä»»åŠ¡
    asyncio.create_task(process_danmaku_batch())
    asyncio.create_task(monitor_queue_status())


# 10. å®šä¹‰è·¯ç”±å’Œå¤„ç†å‡½æ•°
@app.post("/danmaku/add_danmaku", summary="æ¥æ”¶å¼¹å¹•")
async def receive_danmaku(data: DanmakuRequest = Body(...)):
    # æ”¯æŒå¤šç§æ¶ˆæ¯ç±»å‹ï¼šdanmaku, super_chat, gift, buy_guard
    supported_types = {"danmaku", "super_chat", "gift", "buy_guard"}
    if data.danmu_type not in supported_types:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ¶ˆæ¯ç±»å‹: {data.danmu_type}ã€‚æ”¯æŒç±»å‹: {supported_types}")

    # æ„å»ºå¼¹å¹•æ•°æ®
    danmaku_data = {
        'content': data.content,
        'danmu_type': data.danmu_type,
        'timestamp': time.time()
    }

    # åˆ¤æ–­æ¶ˆæ¯ç±»å‹å¹¶å¤„ç†
    if danmaku_queue.is_paid_message(data.danmu_type):
        # ä»˜è´¹æ¶ˆæ¯è¿›å…¥ä»˜è´¹é˜Ÿåˆ—
        await danmaku_queue.put(danmaku_data)
        message_type = "ä»˜è´¹æ¶ˆæ¯"
    else:
        # æ™®é€šæ¶ˆæ¯ç›´æ¥è¿›å…¥ABæ¡¶ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        bucket_system.add_danmaku(danmaku_data)
        message_type = "æ™®é€šæ¶ˆæ¯"

    # ç«‹å³è¿”å›å“åº”ï¼Œä¸ç­‰å¾…å®é™…å¤„ç†å®Œæˆ
    return {
        "success": True,
        "message": f"{message_type}å·²æ¥æ”¶",
        "danmu_type": data.danmu_type
    }


# å¯åŠ¨æ¶ˆè´¹ä»»åŠ¡çš„å‡½æ•°
async def start_consume_task():
    """å¯åŠ¨å¼‚æ­¥æ¶ˆè´¹ä»»åŠ¡"""
    try:
        logger.info("å¯åŠ¨å¼¹å¹•æ¶ˆè´¹ä»»åŠ¡")
        await consume_async()
    except Exception as e:
        logger.error(f"æ¶ˆè´¹ä»»åŠ¡å¯åŠ¨å¤±è´¥: {e}", exc_info=True)


# WebSocketç«¯ç‚¹
@app.websocket("/ws/danmaku")
async def websocket_endpoint(websocket):
    """WebSocketè¿æ¥ç«¯ç‚¹ï¼Œç”¨äºæ¥æ”¶å¼¹å¹•æ¶ˆæ¯"""
    logger.info(f"æ”¶åˆ°WebSocketè¿æ¥è¯·æ±‚ï¼Œå®¢æˆ·ç«¯: {websocket.client}")
    try:
        await websocket.accept()  # æ¥å—WebSocketè¿æ¥
        logger.info("WebSocketè¿æ¥å·²æ¥å—")
        await ws_manager.connect(websocket)
        logger.info(f"WebSocketå®¢æˆ·ç«¯å·²æ·»åŠ åˆ°ç®¡ç†å™¨ï¼Œå½“å‰è¿æ¥æ•°: {len(ws_manager.connections)}")

        while True:
            # ä¿æŒè¿æ¥æ´»è·ƒï¼Œæ¥æ”¶å®¢æˆ·ç«¯æ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
            data = await websocket.receive_text()
            logger.info(f"æ”¶åˆ°WebSocketæ¶ˆæ¯: {data}")
            # å¯ä»¥å¤„ç†å®¢æˆ·ç«¯å‘é€çš„æ¶ˆæ¯ï¼Œè¿™é‡Œåªæ˜¯ä¿æŒè¿æ¥

    except websockets.exceptions.ConnectionClosed:
        logger.info("WebSocketè¿æ¥å·²å…³é—­")
    except Exception as e:
        logger.error(f"WebSocketè¿æ¥é”™è¯¯: {e}", exc_info=True)
    finally:
        await ws_manager.disconnect(websocket)
        logger.info(f"WebSocketå®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œå‰©ä½™è¿æ¥æ•°: {len(ws_manager.connections)}")


# 11. å¯åŠ¨æœåŠ¡çš„å…¥å£
if __name__ == "__main__":

    # ============== è¿™é‡Œæ˜¯å¯æ¶ˆè´¹æ¨¡æ‹ŸæœåŠ¡ï¼Œä¸Šçº¿å»é™¤ ======================
    # import mock_service_simple as mock_service
    #
    # # åˆ›å»ºå¹¶å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ¥è¿è¡Œæ¨¡æ‹ŸæœåŠ¡
    # mock_thread = threading.Thread(
    #     target=mock_service.run_server,
    #     args=(2345,),
    #     daemon=True
    # )
    # mock_thread.start()
    # ============== è¿™é‡Œæ˜¯å¯æ¶ˆè´¹æ¨¡æ‹ŸæœåŠ¡ï¼Œä¸Šçº¿å»é™¤ ======================

    # è®¡ç®—å·¥ä½œè¿›ç¨‹æ•°
    if config['workers'] == 'auto':
        workers = min(8, os.cpu_count() * 2 + 1)  # æ ¹æ®CPUæ ¸å¿ƒæ•°è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°
    else:
        workers = int(config['workers'])

    # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
    async def main():
        """ä¸»å¼‚æ­¥å‡½æ•°"""
        # å¯åŠ¨æ¶ˆè´¹ä»»åŠ¡
        consume_task = asyncio.create_task(consume_async())
        logger.info("å¼¹å¹•æ¶ˆè´¹ä»»åŠ¡å·²å¯åŠ¨")

        # å¯åŠ¨uvicornæœåŠ¡
        uvicorn_config = uvicorn.Config(
            "danmaku_proxy:app",
            host="0.0.0.0",  # å…è®¸æ‰€æœ‰IPè¿æ¥
            port=config['port'],
            reload=config['reload'],
            workers=workers,  # ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼ä¾¿äºè°ƒè¯•
            limit_concurrency=config['limit_concurrency'],
            backlog=config['backlog'],
            log_level="info"
        )
        server = uvicorn.Server(uvicorn_config)
        await server.serve()

    # è¿è¡Œä¸»å¼‚æ­¥å‡½æ•°
    asyncio.run(main())