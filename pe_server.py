import time
from typing import List, Optional, Dict, Any
import asyncio
import os
import json
from pathlib import Path

from fastapi import FastAPI
from pydantic import BaseModel, Field
import httpx

from util import load_system_prompt, discover_tools, call_rag, rag_chunks_to_system_prompt, get_history_for_session, \
    estimate_tokens_from_messages, compress_assistant_messages, monitor_task, monitor_thread_task, fetch_session_history
from config_manager import ConfigManager


# é…ç½®åŠ è½½å™¨ï¼ˆä»server.pyå¤åˆ¶è¿‡æ¥ï¼‰
class ConfigLoader:
    _instance = None
    _config = None

    @classmethod
    def get_config(cls, config_path='config.json'):
        if cls._config is None:
            cls._load_config(config_path)
        return cls._config

    @classmethod
    def _load_config(cls, config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)

            # æ‰å¹³åŒ–é…ç½®ï¼Œä¾¿äºè®¿é—®
            cls._config = {
                # æœåŠ¡å™¨é…ç½®
                'port': config_data['server']['port'],
                'workers': config_data['server']['workers'],
                'limit_concurrency': config_data['server']['limit_concurrency'],
                'backlog': config_data['server']['backlog'],
                'reload': config_data['server']['reload'],
                'timeout_keep_alive': config_data['server'].get('timeout_keep_alive', 5),
                # PE Settings
                'pe_enable_history': config_data['pe_settings']['enable_history'],
                'pe_history_max_rounds': config_data['pe_settings']['history_max_rounds'],
                'pe_enable_tools': config_data['pe_settings']['enable_tools'],
                'pe_enable_rag': config_data['pe_settings']['enable_rag'],
                'pe_max_token_budget': config_data['pe_settings']['max_token_budget'],
                'pe_system_prompt_path': config_data['pe_settings']['system_prompt_path'],
                'pe_tool_service_url': config_data['pe_settings']['tool_service_url'],
                'pe_rag_service_url': config_data['pe_settings']['rag_service_url'],
                'pe_rag_top_k': config_data['pe_settings']['rag_top_k'],
                'pe_api_url': config_data['pe_settings']['api_url'],
                'pe_external_service_timeout': config_data['pe_settings'].get('external_service_timeout', 2),
                # è¿æ¥æ± é…ç½®
                'connection_pool_size': config_data.get('connection_pool', {}).get('connection_pool_size', 20),
                'connection_timeout': config_data.get('connection_pool', {}).get('connection_timeout', 2),
                'read_timeout': config_data.get('connection_pool', {}).get('read_timeout', 3),
            }
            print(f"é…ç½®åŠ è½½æˆåŠŸ: {cls._config}")
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
            # ä½¿ç”¨é»˜è®¤é…ç½®ä½œä¸ºåå¤‡
            cls._config = {
                'port': 18080,
                'workers': 1,
                'limit_concurrency': 100,
                'backlog': 512,
                'reload': True,
                'timeout_keep_alive': 5,
                # PE Settings é»˜è®¤å€¼
                'pe_enable_history': True,
                'pe_history_max_rounds': 6,
                'pe_enable_tools': True,
                'pe_enable_rag': True,
                'pe_max_token_budget': 7000,
                'pe_system_prompt_path': "systemPrompt.json",
                'pe_api_url': "/api/build_prompt",
                'pe_tool_service_url': "http://localhost:8000/tool/get_tool_list",
                'pe_rag_service_url': "http://localhost:8000/rag/query_and_embedding",
                'pe_rag_top_k': 3,
                'pe_external_service_timeout': 2,
                # è¿æ¥æ± é…ç½®
                'connection_pool_size': 20,
                'connection_timeout': 2,
                'read_timeout': 3,
            }
            print(f"ä½¿ç”¨é»˜è®¤é…ç½®: {cls._config}")

app = FastAPI(title="Prompt Engine (PE) - FastAPI")
# è·å–é…ç½®
config = ConfigManager.get_config()

# åˆ›å»ºHTTPå®¢æˆ·ç«¯è¿æ¥æ± 
httpx_client = httpx.AsyncClient(
    limits=httpx.Limits(
        max_connections=config.get('connection_pool_size', 20),
        max_keepalive_connections=config.get('connection_pool_size', 20),
        keepalive_expiry=30
    ),
    timeout=httpx.Timeout(
        connect=config.get('connection_timeout', 2),
        read=config.get('read_timeout', 3),
        write=config.get('read_timeout', 3),
        pool=config.get('connection_timeout', 2)
    )
)

# ======= è¯·æ±‚ / å“åº” æ¨¡å‹ ========
class BuildRequest(BaseModel):
    session_id: Optional[str] = Field(None, description="ä¼šè¯IDï¼Œç”¨äºä»ç¼“å­˜ä¸­è·å–å†å²")
    user_query: str = Field(..., description="ç”¨æˆ·å½“å‰ query")


class BuildResponse(BaseModel):
    llm_request: Dict[str, Any]
    estimated_tokens: int
    trimmed_history_rounds: int


# ======= API Endpoint: æäº¤å¹¶ç”Ÿæˆ LLM è¯·æ±‚ä½“ =======
@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶çš„é¢„çƒ­äº‹ä»¶"""
    print("ğŸš€ PE Server æ­£åœ¨å¯åŠ¨...")
    await warmup_services()
    print("âœ… PE Server å¯åŠ¨å®Œæˆ")


@app.post("/api/build_request", response_model=BuildResponse)
async def build_request(req: BuildRequest):
    session_id = req.session_id
    user_query = req.user_query
    start_time = time.time()

    try:
        # å¹¶è¡Œè·å– system promptã€toolsã€ragï¼ˆä½¿ç”¨è¿æ¥æ± å’Œè¶…æ—¶æ§åˆ¶ï¼‰
        tasks = []
        
        # system promptåŠ è½½ï¼ˆçº¿ç¨‹æ± ä»»åŠ¡ï¼‰
        tasks.append(asyncio.to_thread(get_warmup_system_prompt, config['pe_system_prompt_path']))
        
        # toolså‘ç°ï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
        if config['pe_enable_tools']:
            tasks.append(discover_tools(httpx_client))
        else:
            tasks.append(asyncio.create_task(asyncio.sleep(0)))  # ç©ºä»»åŠ¡
        
        # ragè°ƒç”¨ï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
        if config['pe_enable_rag']:
            tasks.append(call_rag(httpx_client, user_query, config['pe_rag_top_k']))
        else:
            tasks.append(asyncio.create_task(asyncio.sleep(0)))  # ç©ºä»»åŠ¡
        
        # ä¼šè¯å†å²è·å–ï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
        if config['pe_enable_history']:
            tasks.append(fetch_session_history(httpx_client, session_id))
        else:
            tasks.append(asyncio.create_task(asyncio.sleep(0)))  # ç©ºä»»åŠ¡
        
        # è®¾ç½®è¶…æ—¶æ§åˆ¶
        timeout_seconds = config.get('pe_external_service_timeout', 2)
        system_prompt, tools_list, rag_results, external_history = await asyncio.wait_for(
            asyncio.gather(*tasks, return_exceptions=True),
            timeout=timeout_seconds
        )
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        if isinstance(system_prompt, Exception):
            print(f"System prompt loading failed: {system_prompt}")
            system_prompt = None
        if isinstance(tools_list, Exception):
            print(f"Tools discovery failed: {tools_list}")
            tools_list = []
        if isinstance(rag_results, Exception):
            print(f"RAG call failed: {rag_results}")
            rag_results = []
        if isinstance(external_history, Exception):
            print(f"Session history fetch failed: {external_history}")
            external_history = None
            
    except asyncio.TimeoutError:
        print(f"External services timeout after {timeout_seconds}s")
        system_prompt = None
        tools_list = []
        rag_results = []
        external_history = None
    except Exception as e:
        print(f"Error in external service calls: {e}")
        system_prompt = None
        tools_list = []
        rag_results = []
        external_history = None

    # messages é¡ºåºï¼šsystem, rag(system), history..., user
    messages: List[Dict[str, str]] = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    # ä½¿ç”¨é¢„çƒ­çš„system promptï¼ˆå¦‚æœå¯ç”¨ï¼‰
    elif preloaded_system_prompt:
        messages.append({"role": "system", "content": preloaded_system_prompt})

    rag_system_msg = rag_chunks_to_system_prompt(rag_results)
    if rag_system_msg:
        messages.append(rag_system_msg)

    # attach history (å— setting æ§åˆ¶)
    trimmed_history_rounds = config['pe_history_max_rounds']
    # ä¼˜å…ˆä½¿ç”¨ä»å¤–éƒ¨æœåŠ¡è·å–çš„å†å²è®°å½•ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æœ¬åœ°å†…å­˜å†å²
    if external_history:
        history_messages = external_history[-trimmed_history_rounds*2:]  # æ¯æ¡å¯¹è¯åŒ…å«userå’Œassistantä¸¤æ¡æ¶ˆæ¯
    else:
        history_messages = get_history_for_session(session_id, trimmed_history_rounds)
    messages.extend(history_messages)

    # æœ€ååŠ å…¥å½“å‰ user query
    messages.append({"role": "user", "content": user_query})

    # ä¼°ç®— token
    estimated_tokens = estimate_tokens_from_messages(messages)

    # å‰ªè£å†å²ç›´åˆ°æ»¡è¶³é¢„ç®—
    trimmed_rounds = trimmed_history_rounds
    while estimated_tokens > config['pe_max_token_budget'] and trimmed_rounds > 0:
        trimmed_rounds -= 1
        # ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨å†å²è®°å½•è¿›è¡Œè£å‰ª
        if external_history:
            history_messages = external_history[-trimmed_rounds*2:]
        else:
            history_messages = get_history_for_session(session_id, trimmed_rounds)
        # å¯é€‰ï¼šå¯¹ assistant æ¶ˆæ¯è¿›è¡Œå‹ç¼©
        history_messages = compress_assistant_messages(history_messages, target_chars=600)
        # rebuild messages
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if rag_system_msg:
            messages.append(rag_system_msg)
        messages.extend(history_messages)
        messages.append({"role": "user", "content": user_query})
        estimated_tokens = estimate_tokens_from_messages(messages)

    # æ„å»ºæœ€ç»ˆ LLM è¯·æ±‚ä½“ï¼ˆç¬¦åˆ OpenAI Chat styleï¼‰
    llm_request = {
        # æ¨¡å‹åœ¨ Agent-Coreä¸­é…ç½®
        "messages": messages,
        "tools": tools_list if config['pe_enable_tools'] else [],
        "max_tokens": config['pe_max_token_budget'],
    }

    processing_time = (time.time() - start_time) * 1000
    print(f"Request processed in {processing_time:.2f}ms")

    return BuildResponse(
        llm_request=llm_request,
        estimated_tokens=estimated_tokens,
        trimmed_history_rounds=trimmed_rounds,
    )


# ======= API: ç®€å•çš„ä¼šè¯å†å²ç®¡ç†ï¼ˆä»…ç¤ºä¾‹ï¼‰ =======
class AppendMessageReq(BaseModel):
    session_id: str
    role: str
    content: str


# ======= å¯åŠ¨ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰ =======
if __name__ == "__main__":
    import uvicorn

    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®å¯åŠ¨æœåŠ¡
    uvicorn.run(
        "pe_server:app",
        host="0.0.0.0", 
        port=config['port'], 
        workers=config.get('workers', 1),
        limit_concurrency=config.get('limit_concurrency', 100),
        backlog=config.get('backlog', 512),
        reload=config.get('reload', True),
        timeout_keep_alive=config.get('timeout_keep_alive', 5),
    )
